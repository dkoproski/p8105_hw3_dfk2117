---
title: "p8105_hw3_dfk2117"
author: "Dylan Koproski"
date: "2023-10-08"
output: github_document
---
## Requried Data and Libraries
```{r}
library(p8105.datasets)
library(tidyverse)
library(patchwork)
library(ggridges)
data("instacart")
data("brfss_smart2010")
```

## Problem 1

### First, putting the data into a tibble

```{r}

instacart = 
  instacart |> 
  as_tibble()

```
This chunk takes the `instacart` data from the `p8105.datasets` and saves it as a tibble that I can manipulate later. This dataframe contains nearly 1.4 million observations of 15 variables. These variables include but are not limited to order id's for each order, the id of the product ordered, whether or not the item was reordered, the time of the order (split in day of week, and hour).

### How many aisles are there, and which aisles are the most items ordered from?

```{r}

df_aisles = 
  instacart |> 
  group_by(aisle) |> 
  summarise(n_aisle = n()) |> 
  arrange(desc(n_aisle))

df_aisles
```

Using the `group_by()` function and the `summarise()` function, we see that there are 134 different aisles. Further, using `arrange()`, we can see that 83, 24,123,120,21 are the 5 most ordered-from aisles. These correspond to the fresh vegetables, fresh fruits, packaged vegetable fruits, yogurt and packaged cheese aisles. I feel like the fresh fruits and fresh vegetable aisles have a disproportionately large number of items because produce is usually sold individually, so maybe each individual fruit/vegetable is being counted as an item.

### Number of items ordered in each aisle

```{r}
instacart |> 
  count(aisle) |> 
  filter(n > 10000) |> 
  mutate(aisle = fct_reorder(aisle, n)) |> 
  ggplot(aes(x = aisle, y = n, color = aisle)) + 
  geom_point() + 
  labs(title = "Number of Items Purchased per Aisle",
       x = "Response",
       y = "Data Value") + 
  theme(axis.text.x = element_text(size = 5)) +
  theme(axis.text.x = element_text(angle = 40, hjust = 1)) +
  guides(color = FALSE)
```

The above code first uses `count()` to count the number of items ordered from each aisle, then `filter()` restricts this value to only those greater than 10,000 items. The `fct_reorder()` function is then used inside of `mutate()` to reordee the `aisle` variable accordig to the new `n` variable. The remaining code creates a ggplot using `geom_point()` with `x = aisle` and `y = n`. The remaining code is aesthetics to make the plot more human-readable. Fresh vegetables and fresh fruits are the highest sellers by a large margin.

### Three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”

```{r}

# I did this one very wrong the first go around, I pretty much got just to the mutate step, I corrected it based on the homework notes
instacart |> 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) |>
  group_by(aisle) |> 
  count(product_name) |> 
  mutate(rank = min_rank(desc(n))) |> 
  filter(rank < 4) |> 
  arrange(desc(n)) |>
  knitr::kable()

```

This code first filters out the three aisles of interest, then uses `group_by()` to ensure that the `count(product_name)` function counts products within each aisle. Next, a new `rank` variable is made with `mutate()` to rank, in descending order, the most popular items for aisle. Then the top 3 ranks are filtered and arranged in descending order. The result is a table that is not reader friendly, so `knitr::kable()` is used to make it readable. Light Brown Sugar is the most popular item in the baking aisle, Snack Sticks Chicken & Rice Recipe Dog Treats is the most popular in the dog food care aisle and Organic Baby Spinach is the most popular in packaged vegetables fruits.

### Mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week

```{r}
instacart |>
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) |>
  group_by(product_name, order_dow) |>
  summarize(mean_hour = mean(order_hour_of_day)) |>
  pivot_wider(
    names_from = order_dow, 
    values_from = mean_hour) |>
  knitr::kable(digits = 2)
```

`filter()` is used to only show the `Pink Lady Apple` and `Coffee Ice Cream` product names. Then we `group_by()` the product name, and the order day of the week. Then `summarize()` calculates the `mean(order_hour_of_day)` based on the grouped parameters. Next `pivot_wider()` takes the table we have and effectively flips it on its side so we have the `order_dow` as the top row of the table. `knitr::kable()` is then used with `digits = 2` to make a reader friendly table rounded to 2 digits. It looks like both coffee ice cream and pink lady apples are order the latest in the day on average on tuesday and wednesday respectively.

## Problem 2

### Cleaning the dataframe

```{r}
df_brfss =
  brfss_smart2010 |> 
  janitor::clean_names() |> 
  rename(state_abbr = locationabbr,
         state_county = locationdesc) |> 
  filter(topic == "Overall Health") |> 
  filter(response %in% c("Excellent", "Fair", "Very good", "Good", "Poor")) |> 
  mutate(response = factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"), ordered = TRUE))
```

Here I make a dataframe called `df_brfss` so that I do not alter the original dataframe. I used `clean_names()` to tidy the variable names, but I also had to use the `rename()` function on `locationabbr` and `locationdesc` since their names were not intuitive and difficult to read. I used the `filter()` function two times, once to filter only the `Overall Health` topic and the other to filter the specified `responses`. Finally, I made response into a factor variable with levels Excellent - Poor in descending order using `mutate()`. The resulting dataset has 10,625 observations of 23 variables. These include, but are not limited to data_value, location variables (including the state abbreviation and state county) a response variable ranging from poor to excellent and the sample_size.

### In 2002, which states were observed at 7 or more locations? What about in 2010?

```{r}
n_state_2002 =
  df_brfss |> 
  filter(year == 2002) |> 
  group_by(state_abbr) |> 
  count(geo_location) |> 
  count(state_abbr) |> 
  filter(n >= 7)

n_state_2010 =
  df_brfss |> 
  filter(year == 2010) |> 
  group_by(state_abbr) |> 
  count(geo_location) |> 
  count(state_abbr) |> 
  filter(n >= 7)

n_state_2002
n_state_2010
```
I interpreted this question as looking for the number of states (`state_abbr`) that contain observations from 7 or more unique `geo_location`. I did this by first using `filter()` to focus on the correct year, then I used the `group_by()` function to group the dataset by state abbreviation. After this, I used `count()` twice, first to count the occurrence of each unique `geo_location` observation within each state, then another time to count the occurrence of each `state_abbr` corresponding to each `geo_location`. Finally, I `filter()` once more to only focus on states with greater than 7 locations. The result is a count of unique `geo_location` observations within each `state_abbr`. In 2002, 6 states were observed at 7 or more geographic locations. In 2010, 14 states were observed at 7 or more geographic locations.

### Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).

```{r}
df_brfss |> 
  filter(response == 'Excellent') |> 
  group_by(year, state_abbr) |> 
  summarise(avg_dv_pct = mean(data_value)) |> 
  ggplot(aes(x = year, y = avg_dv_pct, group = state_abbr, color = state_abbr)) +
  geom_line() +
  labs(title = "Average Data Value of Excellent Responses Over Years by State",
       x = "Year",
       y = "Average Data Value (%)") +
  theme(legend.position = "right") +
  theme(legend.key.size = unit(1, 'cm'),
        legend.key.height = unit(0.5, 'cm'), 
        legend.key.width = unit(0.5, 'cm'), 
        legend.title = element_text(size=7),
        legend.text = element_text(size=7)) +
  scale_color_discrete(name="")

```

The above code takes the tidied dataset I made in the data manipulation step, Uses `filter()` to focus on only excellent responses, uses `group_by()` to group the year and abbreviated state name variables. I then use `summarise()` to calculate the average data value in these groups. Next, I make a `geom_line()` plot with year along the x-axis and the average data percent on the y - axis. I group by and color by the abbreviated state name so that each state gets its own individual colored line. I also made some tweaks to the legend using arguments I found on stackoverflow because it was too intrusive and was taking away from the main plot. This plot is hard to read as there are so many states and the colors begin to blend.

### Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

```{r}
df_brfss |> 
  filter(year %in% c(2006, 2010), 
         state_abbr == "NY", 
         response %in% c("Poor", "Fair", "Good", "Very good", "Excellent")) |> 
  ggplot(aes(x = response, y = data_value, group = geo_location, color = geo_location)) +
  geom_line() +
  facet_wrap(~year) +
  labs(title = "Data Value for Poor-Excellent Responses in NY State",
       x = "Response",
       y = "Data Value") +
  theme(axis.text.x = element_text(size = 7)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
    theme(legend.position = "right") +
  theme(legend.key.size = unit(0.5, 'cm'),
        legend.key.height = unit(0.5, 'cm'), 
        legend.key.width = unit(0.5, 'cm'), 
        legend.title = element_text(size=7),
        legend.text = element_text(size=7))

```

To accomplish this, I first use `filter()` with three arguments: the years 2006 and 2010, the state New York, and responses from `poor` to `excellent`. I then use `geom_line()` with response on the x-axis and the data value on the y-axis. This creates one plot with the desired information (namely data value vs. response with lines for each location), but it combines the two years. To remedy this, `facet_wrap(~year)` is used which creates an individual plot for each year. The remaining code focuses on the aesthetics as I attempt to make the plot more reader friendly by changing the size, shape and angle of text. It looks like there is an incrase in `very good` responses in 2010 when compared to 2006. Both years have comparable amounts of poor, fair, good and excellent responses, though.

## Problem 3

### Load, tidy, merge, and otherwise organize the data sets. Your final dataset should include all originally observed variables; exclude participants less than 21 years of age, and those with missing demographic data; and encode data with reasonable variable classes (i.e. not numeric, and using factors with the ordering of tables and plots in mind).



```{r}
df_covar = 
  read_csv("data/nhanes_covar.csv", skip = 4) |> 
  janitor::clean_names() |> 
  mutate(sex = 
           case_match(sex,
                      1 ~ "male",
                      2 ~ "female")) |> 
  mutate(education = 
           case_match(education,
                      1 ~ "less_than_hs",
                      2 ~ "equivalent_hs",
                      3 ~ "more_than_hs")) |>
  mutate(education = factor(education, levels = c("less_than_hs", 
                                                  "equivalent_hs", 
                                                  "more_than_hs"), 
                                                  ordered = TRUE)) |> 
  filter(age >= 21) |> 
  drop_na(sex, age, bmi, education)

df_accel = 
  read_csv("data/nhanes_accel.csv") |> 
  janitor::clean_names()

df_merged =
  inner_join(df_accel, df_covar) 


```

My first step in tidying the data is to `read_csv()` with the `skip = 4` argument since the first 4 rows of the data explain what the numerical levels correspond to. I then clean names. Next, I use `mutate()` three times. The first time I convert the sex variable to male and female based on the numerical levels. I do the same for the education variable changing the three numerical levels to accurate descriptions of the education level. I finally mutate the education variable using `factor()` to order the levels from lowest to highest education level. Finally I `filter()` by age greater than or equal to 21 and drop the requested demographic NAs. I do a simpler process for the accelerometer dataset as it does not require much tidying. I considered `pivot_longer()` here to collapse the `min1:min1440` variables into one variable with names to `time_interval` and values to `accel_value`, but this created a very unweildy dataset that lagged when I manipulated it. I instead just merged the datasets as they were using `inner_join`. The resuling data set has 228 observations of 1445 variables. This is deceiving, though, as only six of these are unique: seqn, sex, age, bmi, education and min1-min1440 (which are effectively one variable).

### Produce a reader-friendly table for the number of men and women in each education category, and create a visualization of the age distributions for men and women in each education category. Comment on these items.

```{r}
 df_merged |> 
  group_by(sex, education) |> 
  summarise(count = n()) |> 
  knitr::kable()

```

This creates a table with the counts of each education level within each sex. Then `knitr::kable()` makes the table more readable.

```{r}
df_merged |> 
  ggplot(aes(x = education, fill = sex)) +
  geom_bar() +
    labs(
      title = "Education Level by Sex",
       x = "Education Level",
       y = "# of Individuals") +
  facet_wrap(~sex) +
  scale_x_discrete(labels = c(equivalent_hs = "High School Equivalent", "less_than_hs" = "Less than High School", "more_than_hs" = "More than High School")) +
  theme(axis.text.x = element_text(size = 7)) +
  theme(axis.text.x = element_text(angle = 20, hjust = 1))

```

This makes a `geom_bar()` plot with x as education and colored by sex. This plot would normally be a stacked bar plot, but using `facet_wrap()` by `~sex` it turns into a side by side bar plot with one graph for each sex. Next, I change the labels to make them more descriptive and change the sizes and angle of the text to make them reader friendly. Comparable amounts of individuals have more than high school for both sexes, but it looks like more males have a high school equivalent than females and more females have less than high school than males.

### Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate across minutes to create a total activity variable for each participant. Plot these total activities (y-axis) against age (x-axis); your plot should compare men to women and have separate panels for each education level. Include a trend line or a smooth to illustrate differences. Comment on your plot.


```{r}
df_merged |> 
  group_by(seqn) |> 
  mutate(total_activity = 
           sum(
             across(
               starts_with("min")))) |>
  ggplot(aes(x = age, y = total_activity, color = sex)) + 
  geom_point(alpha=0.6) + 
  geom_smooth(method = "loess", se = FALSE) +
  facet_wrap(~education, scales = "free") +
  labs(title = "Total Activity by Age in each Education Level", x = "Age (years)", y = "Total Activity") 

```

I had some trouble getting this one to work properly, but ultimately my main issue was that I didn't `group_by(seqn)` initially. I mutate a new total_activity variable as a sum across the row of all variables starting with `min`. The resulting variable has the sum of each of the 1440 activity values corresponding to each minute for each individual `seqn`. Next I make a ggplot with `geom_point()` set to an alpha of 0.6 and `geom_smooth()` with method `loess` as I feel that it was more descriptive than `lm`. I then `facet_wrap()` by education and change the axis title. It looks like males of age 40+ have higher activity than females in the less than high school education level, but females have higher activity than makes everywhere else. Activity decreases with age in all categories, but in the more than high school category this decrease is much less pronounced.

### Accelerometer data allows the inspection activity over the course of the day. Make a three-panel plot that shows the 24-hour activity time courses for each education level and use color to indicate sex. Describe in words any patterns or conclusions you can make based on this graph; including smooth trends may help identify differences.

```{r}

```






